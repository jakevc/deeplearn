{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Associated Domain boundary prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to reproduce results of the recent paper, [Henderson et al, 2019](https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkz315/5485073#supplementary-data), where the group was able to predict high resolution Topologically Associated Domain (TAD) boundaries from sequencing data using a convolutional neural network. After model fine-tuning and optimization they were able to detect boundaries with an accuracy of 96% from one-hot encoded sequencing data alone. Here I will attempt to reproduce their results using the [fastai](https://docs.fast.ai/) library, built on [PyTorch](https://pytorch.org/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A, T, C and G were encoded as [1, 0, 0,\n",
    "0], [0, 1, 0, 0], [0, 0, 1, 0] and [0, 0, 0, 1],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each sequence is converted to a matrix of shape (L,4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D convolutions using kernels of shape (k, 4). Yilded activation map ((L+2(p – d)(k – 1) – 1)/s, 4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data were cloned from the public repository, https://github.com/lincshunter/TADBoundaryDectector and unpacked with the `unrar` utility in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = Path(\"data/TADBoundaryDectector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/TADBoundaryDectector/.git'),\n",
       " PosixPath('data/TADBoundaryDectector/Models.py'),\n",
       " PosixPath('data/TADBoundaryDectector/README.md'),\n",
       " PosixPath('data/TADBoundaryDectector/dm3.kc167.example.rar'),\n",
       " PosixPath('data/TADBoundaryDectector/dm3.kc167.example.h5')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpath.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file = \"data/TADBoundaryDectector/dm3.kc167.example.h5\"\n",
    "f = h5py.File(h5file, 'r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['x_test', 'x_train', 'x_val', 'y_test', 'y_train', 'y_val']>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys() # list available keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.array(f['x_test'])\n",
    "x_test.shape # a 1k by 1k by 4 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(f['x_train'])\n",
    "x_val = np.array(f['x_val']) \n",
    "y_test = np.array(f['y_test'])\n",
    "y_train = np.array(f['y_train'])\n",
    "y_val = np.array(f['y_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28127,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cooerce these data into a fastai databunch object. Convert all to tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,x_train,x_val,y_test,y_train,y_val = map(torch.tensor, [x_test,x_train,x_val,y_test,y_train,y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1000, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use minibatches\n",
    "bs=50\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_val, y_val)\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "data = DataBunch.create(train_ds, valid_ds, test_ds, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 1000, 4]), torch.Size([50]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=next(iter(data.train_dl))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 1]), torch.Size([50, 1000, 4]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=data.one_batch()\n",
    "y,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 4, 1000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch, chan, len\n",
    "x.shape[1]\n",
    "x.transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model architecture like [three_CNN_LSTM](https://github.com/lincshunter/TADBoundaryDectector/blob/master/Models.py)\n",
    "\n",
    "\n",
    "pseudocode of the three_CNN_LSTM model:\n",
    "\n",
    "\n",
    " - sequential\n",
    " - conv1d\n",
    " - relU\n",
    " - dropout(0.2)\n",
    " - maxpool1d\n",
    "\n",
    "\n",
    " - conv1d\n",
    " - relu\n",
    " - relu\n",
    " - relu\n",
    " - dropout 0.3\n",
    " - maxpool1d \n",
    "\n",
    "\n",
    " - conv1d\n",
    " - relu\n",
    " - dropout(0.3)\n",
    " - maxpool1d \n",
    "\n",
    "\n",
    " - bidirectionalLSTM\n",
    " - flatten \n",
    " - dense\n",
    " - sigmoid\n",
    "\n",
    "\n",
    "using ADAM optimizaiton with cross entropy loss\n",
    "\n",
    "\n",
    "\n",
    "This might be helpful \n",
    "\n",
    "https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT PARAMETERS\n",
    "s = 1 # stride \n",
    "p = 1 # padding \n",
    "d = 0 # dilation\n",
    "bs = 50 # batch size \n",
    "lr = 1e-3 \n",
    "inshape = x_train.shape[1:3] \n",
    "ks = 9 # kernel size\n",
    "nk = 64 # n kernels\n",
    "lstmU = 40 # LSTM units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "((L+2(p – d)(k – 1) – 1)/s, 4), the resulting size of a 1d convolution on the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1015, 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1000+2*(9-1) - 1, 4) # after first conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 4)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1015+2*(9-1) - 1, 4) # after second conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1045, 4)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1030+2*(9-1) - 1, 4) # after thrid conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 4])\n"
     ]
    }
   ],
   "source": [
    "print(inshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# epochs = 150 \n",
    "# kernsize = 9 \n",
    "# bs = 50\n",
    "# verbose = 1 \n",
    "# nclass = 2\n",
    "# metrics = ['accuracy']\n",
    "# loss = 'binary_crossentropy'\n",
    "# kernel = 'glorot_uniform'\n",
    "\n",
    "class BoundaryModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(4,nk,ks,padding=1) # inchan # outchan # len \n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size = 5,\n",
    "                                 stride = 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64,nk,ks,padding=1) \n",
    "        self.drop2 = nn.Dropout(p=0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size = 5,\n",
    "                                 stride = 2)\n",
    "        \n",
    "        # need to have this up here not in the forward\n",
    "        self.bgru = nn.GRU(117, 40, bidirectional=True)\n",
    "        self.lin = nn.Linear(64,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # permute shape from [batch, len, chan] to [batch, chan, len]\n",
    "        x = torch.Tensor.permute(x,0,2,1)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        print(f\"Size after first pool is {x.size()}\")\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        print(f\"Size after second pool is {x.size()}\")\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        print(f\"Size after third pool is {x.size()}\")\n",
    "        \n",
    "        _, hn = self.bgru(x)\n",
    "        print(f\"Size after bidirectional GRU is {hn.size()}\")\n",
    "        x = hn.view(-1, x.size(1))\n",
    "        print(f\"Size after flatten is {x.size()}\")\n",
    "        x = self.lin(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        print(f\"Final Size: {x.size()}\")\n",
    "        \n",
    "        return self.sigmoid(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after first pool is torch.Size([50, 64, 495])\n",
      "Size after second pool is torch.Size([50, 64, 243])\n",
      "Size after third pool is torch.Size([50, 64, 117])\n",
      "Size after bidirectional GRU is torch.Size([2, 64, 40])\n",
      "Size after flatten is torch.Size([80, 64])\n",
      "Final Size: torch.Size([80, 1])\n"
     ]
    }
   ],
   "source": [
    "m = BoundaryModel().cuda()\n",
    "data1_mod = m(x.type(torch.FloatTensor).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_mod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, m, loss_func = nn.CrossEntropyLoss(), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method model_summary of Learner(data=DataBunch;\n",
       "\n",
       "Train: <torch.utils.data.dataset.TensorDataset object at 0x7f55bab4e470>;\n",
       "\n",
       "Valid: <torch.utils.data.dataset.TensorDataset object at 0x7f55bab4e4a8>;\n",
       "\n",
       "Test: <torch.utils.data.dataset.TensorDataset object at 0x7f55bab4e4e0>, model=BoundaryModel(\n",
       "  (conv1): Conv1d(4, 64, kernel_size=(9,), stride=(1,), padding=(1,))\n",
       "  (drop1): Dropout(p=0.2)\n",
       "  (relu): ReLU()\n",
       "  (pool): MaxPool1d(kernel_size=5, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(1,))\n",
       "  (drop2): Dropout(p=0.3)\n",
       "  (bgru): GRU(117, 40, bidirectional=True)\n",
       "  (lin): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=CrossEntropyLoss(), metrics=[<function accuracy at 0x7f55d269cb70>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Conv1d(4, 64, kernel_size=(9,), stride=(1,), padding=(1,))\n",
       "  (1): Dropout(p=0.2)\n",
       "  (2): ReLU()\n",
       "  (3): MaxPool1d(kernel_size=5, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(1,))\n",
       "  (5): Dropout(p=0.3)\n",
       "  (6): GRU(117, 40, bidirectional=True)\n",
       "  (7): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (8): Sigmoid()\n",
       ")], add_time=True, silent=False)>"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
